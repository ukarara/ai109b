# W13上課筆記
* [梯度下降法](http://programmermedia.org/root/%E9%99%B3%E9%8D%BE%E8%AA%A0/%E8%AA%B2%E7%A8%8B/%E4%BA%BA%E5%B7%A5%E6%99%BA%E6%85%A7/07-neural/02-gradient/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95.md)  
如果我們朝著《斜率最大》方向的《正梯度》走，那麼就會愈走愈高，但是如果朝著《逆梯度》方向走，那麼就會愈走愈低。  
而梯度下降法，就是朝著《逆梯度》的方向走，於是就可以不斷下降，直到到達梯度為 0 的點 (斜率最大的方向仍然是斜率為零)，此時就已經到了一個《谷底》，也就是區域的最低點了！
* [Hacker's guide to Neural Networks](http://karpathy.github.io/neuralnets/)  

* [反傳遞演算法](http://programmermedia.org/root/%E9%99%B3%E9%8D%BE%E8%AA%A0/%E8%AA%B2%E7%A8%8B/%E4%BA%BA%E5%B7%A5%E6%99%BA%E6%85%A7/07-neural/03-net/%E5%8F%8D%E5%82%B3%E9%81%9E%E6%BC%94%E7%AE%97%E6%B3%95%E6%89%8B%E7%AE%97%E6%A1%88%E4%BE%8B.md)  
反向傳播（英語：Backpropagation，縮寫為BP）是「誤差反向傳播」的簡稱，是一種與最優化方法（如梯度下降法）結合使用的，用來訓練人工神經網絡的常見方法。該方法對網絡中所有權重計算損失函數的梯度。這個梯度會反饋給最優化方法，用來更新權值以最小化損失函數。  
反向傳播要求有對每個輸入值想得到的已知輸出，來計算損失函數梯度。因此，它通常被認為是一種監督式學習方法，雖然它也用在一些無監督網絡（如自動編碼器）中。它是多層前饋網絡的Delta規則的推廣，可以用鏈式法則對每層迭代計算梯度。反向傳播要求人工神經元（或「節點」）的激勵函數可微。內容來自[WIKI](https://zh.wikipedia.org/wiki/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95)  


* net1.py (ai\07-neural\03-net\net1.py)
>反傳遞演算法在(ai\07-neural\03-net\net.py)
```PS
PS C:\Users\User\Desktop\109-2school\ai\07-neural\03-net> python net1.py 
net.forward()= 10
net.backwward()
x= v:1 g:2 y= v:3 g:6 o= v:10 g:1      
gfx = x.g/o.g =  2.0 gfy = y.g/o.g= 6.0
```
* net2.py (ai\07-neural\03-net\net2.py)
```PS
PS C:\Users\User\Desktop\109-2school\ai\07-neural\03-net> python net2.py 
0  =>  10
1  =>  9.216
2  =>  8.4934656
~
97  =>  0.003635960944957329
98  =>  0.003350901606872675
99  =>  0.003088190920893857
x= 0.01687031935884968 y= 0.050610958076549
```
## Pytorch 套件
[Pytorch中文教程](https://github.com/fendouai/PyTorchDocs)
* 功能
    * 以自動微分為基礎的函示庫
    * 具有强大的GPU加速的张量计算（如Numpy）
    * 包含自动求导系统的深度神经网络  
* 優點  
    * 支持GPU  
    * 灵活，支持动态神经网络  
    * 底层代码易于理解  
    * 命令式体验  
    * 自定义扩展  

* 安裝 Pytorch
```PS
pip install torch
```
>浮點數才能算梯度
1. autograd0.py (ai\07-neural\04-torch)
```PS
PS C:\Users\User\Desktop\109-2school\ai\07-neural\04-torch> python autograd0.py
tensor(2.)      # +" . "代表他是浮點數
tensor(6.)      # +" . "代表他是浮點數
10.0 
```

2. autograd1.py (ai\07-neural\04-torch)
```PS
PS C:\Users\User\Desktop\109-2school\ai\07-neural\04-torch> python autograd1.py
tensor(2.)      # +" . "代表他是浮點數
tensor(6.)      # +" . "代表他是浮點數
10.0
```

3. autograd2.py (ai\07-neural\04-torch)
```PS
PS C:\Users\User\Desktop\109-2school\ai\07-neural\04-torch> python autograd2.py
f= 10.0
x.grad= tensor([2., 6.])        # +" . "代表他是浮點數
```

4. autograd2a.py (ai\07-neural\04-torch)
```PS
PS C:\Users\User\Desktop\109-2school\ai\07-neural\04-torch> python autograd2a.py
f= 3.1622776985168457
x.grad= tensor([0.3162, 0.9487])
```

5. torchGd1.py (ai\07-neural\05-torchGd)
```PS
PS C:\Users\User\Desktop\109-2school\ai\07-neural\05-torchGd> python torchGd1.py
99 x= 0.2755104899406433 loss= 2.9738640785217285
199 x= 0.5883902907371521 loss= 1.9926419258117676
~
4899 x= 1.9998819828033447 loss= 0.0
4999 x= 1.9999058246612549 loss= 0.0
Result: x = 1.999906063079834 loss=0.0
```

6. torchGd2.py (ai\07-neural\05-torchGd)
> 每個參數取微分再加
```PS
PS C:\Users\User\Desktop\109-2school\ai\07-neural\05-torchGd> python torchGd2.py
99 parameters= [tensor(0.5246, requires_grad=True)] loss= 2.1769368648529053
199 parameters= [tensor(0.7923, requires_grad=True)] loss= 1.4586596488952637
~
4599 parameters= [tensor(1.9998, requires_grad=True)] loss= 0.0
4699 parameters= [tensor(1.9999, requires_grad=True)] loss= 0.0
4799 parameters= [tensor(1.9999, requires_grad=True)] loss= 0.0
4899 parameters= [tensor(1.9999, requires_grad=True)] loss= 0.0
4999 parameters= [tensor(1.9999, requires_grad=True)] loss= 0.0
Result: parameters = [tensor(1.9999, requires_grad=True)] loss=0.0
```

7. torchGd3.py (ai\07-neural\05-torchGd)
> 給兩個變數
```PS
PS C:\Users\User\Desktop\109-2school\ai\07-neural\05-torchGd> python torchGd3.py
x= tensor([-0.0018, -0.0035], requires_grad=True)
```

8. regression1train.py (ai\07-neural\06-torchRegression)  
    * 老師Code修改自:[LEARNING PYTORCH WITH EXAMPLES](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html)    

>regression1train.py
```python
# Prepare the input tensor (x, x^2, x^3).
p = torch.tensor([1, 2, 3])
xx = x.unsqueeze(-1).pow(p)
```
>執行
```PS
PS C:\Users\User\Desktop\109-2school\ai\07-neural\06-torchRegression> python regression1train.py
99 1164.6944580078125
199 1058.6097412109375
299 966.8074340820312
~   # 損失減少
9899 5.212013275013305e-05
9999 5.1769664423773065e-05         
Result: y = 0.0004989182343706489 + 1.0004996061325073 x + 2.0004987716674805 x^2 + 3.0004994869232178 x^3      # 值與torch.tensor([1, 2, 3])相當接近
```

9. regression2predict.py (ai\07-neural\06-torchRegression)  
```PS
PS C:\Users\User\Desktop\109-2school\ai\07-neural\06-torchRegression> python regression2predict.py
predict=tensor([33.9925], grad_fn=<ViewBackward>)
```